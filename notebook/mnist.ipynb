{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chokkan/deeplearning/blob/master/notebook/mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sqH2SEO-X9T6"
      },
      "source": [
        "# Neural Networks on MNIST\n",
        "\n",
        "This notebook explains various approaches for implementing neural networks that recognize digits on [MNIST](http://yann.lecun.com/exdb/mnist/) dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OaLeoEcDYVO_"
      },
      "source": [
        "## Preparing the MNIST dataset\n",
        "\n",
        "Most deep learning frameworks provide APIs for loading famous datasets like MNIST (e.g., `torchvision.datasets.MNIST` in pytorch). The APIs are handy, but hide the important step for preparing a training data for a deep learning framework; when graduating from an example dataset to the real data, we must convert a training data of our interest into the data structure that is acceptable by a deep learning framework.\n",
        "\n",
        "The code below downloads the original distribution of the MNIST dataset on the Web, converts the dataset into `numpy` arrays, and saves the arrays as the file `mnist.npz` with keyword names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OMaOcZMBPuQY"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import sys\n",
        "import struct\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "\n",
        "def read_image(fi):\n",
        "    magic, n, rows, columns = struct.unpack(\">IIII\", fi.read(16))\n",
        "    assert magic == 0x00000803\n",
        "    assert rows == 28\n",
        "    assert columns == 28\n",
        "    rawbuffer = fi.read()\n",
        "    assert len(rawbuffer) == n * rows * columns\n",
        "    rawdata = np.frombuffer(rawbuffer, dtype='>u1', count=n*rows*columns)\n",
        "    return rawdata.reshape(n, rows, columns).astype(np.float32) / 255.0\n",
        "\n",
        "def read_label(fi):\n",
        "    magic, n = struct.unpack(\">II\", fi.read(8))\n",
        "    assert magic == 0x00000801\n",
        "    rawbuffer = fi.read()\n",
        "    assert len(rawbuffer) == n\n",
        "    return np.frombuffer(rawbuffer, dtype='>u1', count=n)\n",
        "\n",
        "def openurl_gzip(url):\n",
        "    request = urllib.request.Request(\n",
        "        url,\n",
        "        headers={\n",
        "            \"Accept-Encoding\": \"gzip\",\n",
        "            \"User-Agent\": \"Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11\", \n",
        "        })\n",
        "    response = urllib.request.urlopen(request)\n",
        "    return gzip.GzipFile(fileobj=response, mode='rb')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.savez_compressed(\n",
        "        'mnist',\n",
        "        train_x=read_image(openurl_gzip('https://github.com/SergeiGrudinin/mnist/raw/master/train-images-idx3-ubyte.gz')),\n",
        "        train_y=read_label(openurl_gzip('https://github.com/SergeiGrudinin/mnist/raw/master/train-labels-idx1-ubyte.gz')),\n",
        "        test_x=read_image(openurl_gzip('https://github.com/SergeiGrudinin/mnist/raw/master/t10k-images-idx3-ubyte.gz')),\n",
        "        test_y=read_label(openurl_gzip('https://github.com/SergeiGrudinin/mnist/raw/master/t10k-labels-idx1-ubyte.gz'))\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TStlGwaUaZKC"
      },
      "source": [
        "The file contains four numpy arrays (one tensor and array for each split of training and test sets) with the keywords:\n",
        "\n",
        "+ `train_x`: $60000 \\text{ (images)} \\times 28 \\text{ (y)} \\times 28 \\text{ (x)}$\n",
        "+ `train_y`: $60000 \\text{ (labels)}$\n",
        "+ `test_x`: $10000 \\text{ (images)} \\times 28 \\text{ (y)} \\times 28 \\text{ (x)}$\n",
        "+ `test_y`: $10000 \\text{ (labels)}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "colab_type": "code",
        "id": "2RbqOtJsZH35",
        "outputId": "2c0a0dff-ee73-4e64-8709-d3a012dcd546"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "data = np.load('mnist.npz')\n",
        "\n",
        "print(data['train_x'].shape, data['train_x'].dtype)\n",
        "print(data['train_y'].shape, data['train_y'].dtype)\n",
        "print(data['test_x'].shape, data['test_x'].dtype)\n",
        "print(data['test_y'].shape, data['test_y'].dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_EgmYVMP_iKU"
      },
      "source": [
        "## Visualize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "colab_type": "code",
        "id": "Pr_UACmc3Qgc",
        "outputId": "063c1720-0ef3-49ea-aa45-1230daf4b952"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Index number of an instance (change this to view another instance).\n",
        "i = 1\n",
        "\n",
        "data = np.load('mnist.npz')\n",
        "image = data['train_x'][i]\n",
        "label = data['train_y'][i]\n",
        "\n",
        "print(label)\n",
        "f, ax = plt.subplots(figsize=(16, 16))\n",
        "sns.heatmap(image, annot=True, fmt='.1f', square=True, cmap=\"YlGnBu\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YCr4T9-8cTFH"
      },
      "source": [
        "## Perceptron algorithm for single-layer neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "jr3tb8zIP6H3",
        "outputId": "b5596a5d-1242-4bab-99f0-eddbea3f342a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def image_to_vector(X):\n",
        "    X = np.reshape(X, (len(X), -1))         # Flatten: (N x 28 x 28) -> (N x 784)\n",
        "    return np.c_[X, np.ones(len(X))]        # Append 1: (N x 784) -> (N x 785)\n",
        "\n",
        "data = np.load('mnist.npz')\n",
        "Xtrain = image_to_vector(data['train_x'])   # (60000 x 785)\n",
        "Ytrain = data['train_y']                    # (60000)\n",
        "Xtest = image_to_vector(data['test_x'])     # (10000 x 785)\n",
        "Ytest = data['test_y']                      # (10000)\n",
        "\n",
        "W = np.random.randn(10, 28*28+1)            # (10 x 785)\n",
        "\n",
        "eta = 0.001\n",
        "for t in range(100):\n",
        "    # Structured perceptron for updating weights.\n",
        "    num_correct_train = 0\n",
        "    for x, y in zip(Xtrain, Ytrain):\n",
        "        y_pred = np.argmax(np.dot(W, x))\n",
        "        if y_pred != y:\n",
        "            W[y] += x * eta\n",
        "            W[y_pred] -= x * eta\n",
        "        else:\n",
        "            num_correct_train += 1\n",
        "\n",
        "    # Evaluate and store the accuracy on the test set.\n",
        "    num_correct_test = 0\n",
        "    for x, y in zip(Xtest, Ytest):\n",
        "        y_pred = np.argmax(np.dot(W, x))\n",
        "        if y_pred == y:\n",
        "            num_correct_test += 1\n",
        "    \n",
        "    print('#{}: train_accuracy={:.4f}, test_accuracy={:.4f}'.format(\n",
        "        t,\n",
        "        float(num_correct_train) / len(Ytrain),\n",
        "        float(num_correct_test) / len(Ytest)\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jk8DoJxPdXax"
      },
      "source": [
        "## Stochastic gradient descent for single-layer neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "mZtrUqeSROVa",
        "outputId": "ff528eb6-5ff9-462e-9d9a-6278be80d802"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    # We subtract the maximum value to prevent overflow and underflow problems, \n",
        "    # (result of softmax are invariant even if we add/subtract a constant)\n",
        "    ex = np.exp(x - np.max(x))  # Subtract such that the maximum value is one.\n",
        "    return ex / ex.sum(axis=0)\n",
        "\n",
        "def image_to_vector(X):\n",
        "    X = np.reshape(X, (len(X), -1))     # Flatten: (N x 28 x 28) -> (N x 784)\n",
        "    return np.c_[X, np.ones(len(X))]    # Append 1: (N x 784) -> (N x 785)\n",
        "\n",
        "def label_to_onehot(Y, K):\n",
        "    return np.eye(K)[Y]              # e.g., 3 -> [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
        "    \n",
        "data = np.load('mnist.npz')\n",
        "Xtrain = image_to_vector(data['train_x'])       # (60000 x 785)\n",
        "Ytrain = label_to_onehot(data['train_y'], 10)   # (60000 x 10)\n",
        "Xtest = image_to_vector(data['test_x'])         # (10000 x 785)\n",
        "Ytest = data['test_y']                          # (10000) (not one-hot encoding)\n",
        "\n",
        "W = np.random.randn(10, 28*28+1)                # (10 x 785)\n",
        "\n",
        "eta = 0.001\n",
        "for t in range(100):\n",
        "    loss = 0.\n",
        "    num_correct_train = 0\n",
        "    \n",
        "    # Stochastic gradient descent.\n",
        "    for x, y in zip(Xtrain, Ytrain):\n",
        "        y_pred = softmax(np.dot(W, x))\n",
        "        loss += -np.log(y_pred[np.argmax(y)])\n",
        "        W += np.outer(eta * (y - y_pred), x)    # np.outer: out[i,j] = a[i]*b[j]\n",
        "        if np.argmax(y_pred) == np.argmax(y):\n",
        "            num_correct_train += 1\n",
        "    \n",
        "    # Evaluate and store the accuracy on the test set.\n",
        "    num_correct_test = 0\n",
        "    for x, y in zip(Xtest, Ytest):\n",
        "        y_pred = np.argmax(np.dot(W, x))\n",
        "        if y_pred == y:\n",
        "            num_correct_test += 1\n",
        "            \n",
        "    print('#{}: loss={:.2f}, train_accuracy={:.4f}, test_accuracy={:.4f}'.format(\n",
        "        t,\n",
        "        loss / len(Ytrain),\n",
        "        float(num_correct_train) / len(Ytrain),\n",
        "        float(num_correct_test) / len(Ytest)\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "twPUp-bbeww2"
      },
      "source": [
        "## Training with pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QwF495MIe0-e"
      },
      "source": [
        "### Activate TensorBoard\n",
        "\n",
        "Usage from PyTorch: https://pytorch.org/docs/stable/tensorboard.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-dKXB9I6CFsp"
      },
      "outputs": [],
      "source": [
        "# Uncomment these lines when you use TensorBoard.\n",
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir ./runs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xvQg0U8GMUYS"
      },
      "source": [
        "### Convert the numpy arrays into pytorch tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MWIINUezQn6H"
      },
      "outputs": [],
      "source": [
        "def create_dataset(x, y, flatten=False):\n",
        "    if flatten:\n",
        "        # Convert it into a matrix (N [samples], 28*28 [dims])\n",
        "        xt = torch.from_numpy(x).view(len(x), -1)\n",
        "    else:\n",
        "        # Convert it into a 4D tensor (N [samples], 1 [ch], 28 [px], 28 [px])\n",
        "        xt = torch.from_numpy(x).unsqueeze(1)\n",
        "    yt = torch.from_numpy(y).long()\n",
        "    return TensorDataset(xt, yt)\n",
        "\n",
        "def test_model(model, loss_fn, test_loader, device):\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0.\n",
        "    num_correct = 0.\n",
        "    for batch_idx, (x, y) in enumerate(test_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_pred = model(x)\n",
        "        loss += loss_fn(y_pred, y).item()\n",
        "        _, predicted = torch.max(y_pred.data, 1)\n",
        "        num_correct += (predicted == y).sum().item()\n",
        "\n",
        "    model.train()\n",
        "    loss /= len(test_loader.dataset)\n",
        "    num_correct /= len(test_loader.dataset)\n",
        "    return loss, num_correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sr0ejX6YM7ej"
      },
      "source": [
        "### Single-layer neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "VWeH4oiZW_nf",
        "outputId": "d2a301d2-95f4-475a-b621-db8b7902698c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "#device = torch.device(\"cpu\") # Uncomment this to run on CPU\n",
        "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "model = nn.Sequential()\n",
        "model.add_module('fc1', nn.Linear(784, 10, bias=True))\n",
        "print(model)\n",
        "model.to(device)\n",
        "\n",
        "data = np.load('mnist.npz')\n",
        "train_set = create_dataset(data['train_x'], data['train_y'], flatten=True)\n",
        "test_set = create_dataset(data['test_x'], data['test_y'], flatten=True)\n",
        "train_loader = DataLoader(train_set, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=128)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "writer = SummaryWriter(comment=\"single-layer\")\n",
        "for t in range(100):\n",
        "    train_loss = 0.\n",
        "    train_correct = 0\n",
        "    \n",
        "    # Training loop for mini-batches\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "        # Make predictions with the current parameters.\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_pred = model(x)\n",
        "        _, predicted = torch.max(y_pred.data, 1)\n",
        "        train_correct += (predicted == y).sum().item()\n",
        "        \n",
        "        # Compute the loss value.\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Update the parameters.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Compute the average loss and accuracy.\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_correct /= float(len(train_loader.dataset))\n",
        "    \n",
        "    # Evaluate the model on the test set.\n",
        "    test_loss, test_correct = test_model(model, loss_fn, test_loader, device)\n",
        "\n",
        "    # Record loss and accuracy values on the training and test sets.\n",
        "    writer.add_scalars('loss', {'train': train_loss, 'test': test_loss}, t)\n",
        "    writer.add_scalars('accuracy', {'train': train_correct, 'test': test_correct}, t)\n",
        "\n",
        "    # Report progress (comment out the following line when you use TensorBoard).\n",
        "    print('#{}: loss_test={:.4f}, loss_train={:.4f}, acc_test={:.4f}, acc_train={:.4f}'.format(\n",
        "        t,\n",
        "        test_loss,\n",
        "        train_loss,\n",
        "        test_correct,\n",
        "        train_correct\n",
        "    ))\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3tEreUaGItQr"
      },
      "source": [
        "### Three-layer neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "cIaz8ZITRcJ2",
        "outputId": "10396b3b-bb65-490d-b9f8-c6a56d511c0f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "#device = torch.device(\"cpu\") # Uncomment this to run on CPU\n",
        "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "model = nn.Sequential()\n",
        "model.add_module('fc1',      nn.Linear(784, 256))\n",
        "model.add_module('relu1',    nn.ReLU())\n",
        "model.add_module('dropout1', nn.Dropout())\n",
        "model.add_module('fc2',      nn.Linear(256, 256))\n",
        "model.add_module('relu2',    nn.ReLU())\n",
        "model.add_module('dropout2', nn.Dropout())\n",
        "model.add_module('fc3',      nn.Linear(256, 10))\n",
        "print(model)\n",
        "model.to(device)\n",
        "\n",
        "data = np.load('mnist.npz')\n",
        "train_set = create_dataset(data['train_x'], data['train_y'], flatten=True)\n",
        "test_set = create_dataset(data['test_x'], data['test_y'], flatten=True)\n",
        "train_loader = DataLoader(train_set, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=128)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "writer = SummaryWriter(comment=\"two-layer\")\n",
        "for t in range(100):\n",
        "    train_loss = 0.\n",
        "    train_correct = 0\n",
        "    \n",
        "    # Training loop for mini-batches\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "        # Make predictions with the current parameters.\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_pred = model(x)\n",
        "        _, predicted = torch.max(y_pred.data, 1)\n",
        "        train_correct += (predicted == y).sum().item()\n",
        "        \n",
        "        # Compute the loss value.\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Update the parameters.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Compute the average loss and accuracy.\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_correct /= float(len(train_loader.dataset))\n",
        "    \n",
        "    # Evaluate the model on the test set.\n",
        "    test_loss, test_correct = test_model(model, loss_fn, test_loader, device)\n",
        "\n",
        "    # Record loss and accuracy values on the training and test sets.\n",
        "    writer.add_scalars('loss', {'train': train_loss, 'test': test_loss}, t)\n",
        "    writer.add_scalars('accuracy', {'train': train_correct, 'test': test_correct}, t)\n",
        "\n",
        "    # Report progress (comment out the following line when you use TensorBoard).\n",
        "    print('#{}: loss_test={:.4f}, loss_train={:.4f}, acc_test={:.4f}, acc_train={:.4f}'.format(\n",
        "        t,\n",
        "        test_loss,\n",
        "        train_loss,\n",
        "        test_correct,\n",
        "        train_correct\n",
        "    ))\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kjBd7K4lI3W5"
      },
      "source": [
        "### Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "v8H-xg9pom28",
        "outputId": "77992adc-fa3b-4f97-8cdf-95359ba0714b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "#device = torch.device(\"cpu\") # Uncomment this to run on CPU\n",
        "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "class Flatten(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.view(-1, 512)\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Conv2d(1, 16, (5, 5)),\n",
        "    torch.nn.MaxPool2d(2),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(),\n",
        "    torch.nn.Conv2d(16, 32, (5, 5)),\n",
        "    torch.nn.MaxPool2d(2),    \n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(),\n",
        "    Flatten(),\n",
        "    torch.nn.Linear(512, 256),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(),\n",
        "    torch.nn.Linear(256, 10),\n",
        ")\n",
        "print(model)\n",
        "model.to(device)\n",
        "\n",
        "data = np.load('mnist.npz')\n",
        "train_set = create_dataset(data['train_x'], data['train_y'])\n",
        "test_set = create_dataset(data['test_x'], data['test_y'])\n",
        "train_loader = DataLoader(train_set, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=128)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "writer = SummaryWriter(comment=\"CNN\")\n",
        "for t in range(100):\n",
        "    train_loss = 0.\n",
        "    train_correct = 0\n",
        "    \n",
        "    # Training loop for mini-batches\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "        # Make predictions with the current parameters.\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_pred = model(x)\n",
        "        _, predicted = torch.max(y_pred.data, 1)\n",
        "        train_correct += (predicted == y).sum().item()\n",
        "        \n",
        "        # Compute the loss value.\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Update the parameters.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Compute the average loss and accuracy.\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_correct /= float(len(train_loader.dataset))\n",
        "    \n",
        "    # Evaluate the model on the test set.\n",
        "    test_loss, test_correct = test_model(model, loss_fn, test_loader, device)\n",
        "\n",
        "    # Record loss and accuracy values on the training and test sets.\n",
        "    writer.add_scalars('loss', {'train': train_loss, 'test': test_loss}, t)\n",
        "    writer.add_scalars('accuracy', {'train': train_correct, 'test': test_correct}, t)\n",
        "\n",
        "    # Report progress (comment out the following line when you use TensorBoard).\n",
        "    print('#{}: loss_test={:.4f}, loss_train={:.4f}, acc_test={:.4f}, acc_train={:.4f}'.format(\n",
        "        t,\n",
        "        test_loss,\n",
        "        train_loss,\n",
        "        test_correct,\n",
        "        train_correct\n",
        "    ))\n",
        "\n",
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "mnist.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
